{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Metaflow Scaling and Production\n",
    "\n",
    "### Metaflow on GCP\n",
    "\n",
    "The true power of Metaflow comes in when we decide to use a different *compute layer*, that is, instead of using our laptop, we send the computations to AWS, GCP, Azure, or K8s. To set up Metaflow on any one of these cloud platforms, there are instructions [here](https://outerbounds.com/engineering/welcome/). For this demo, I will be using GCP. \n",
    "\n",
    "To use GCP, here is an overview of the steps, many of which you should already have completed from previous labs. \n",
    "\n",
    "Install Tools:  \n",
    "- Install Terraform (infrastructure-as-code tool) [here](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)  \n",
    "- Install the GCP CLI [here](https://cloud.google.com/sdk/docs/install-sdk)  \n",
    "- Install kubectl [here](https://kubernetes.io/docs/tasks/tools/#kubectl)  \n",
    "- Install kubernetes python library in your environment `pip install kubernetes`\n",
    "\n",
    "Clone Repository with Terraform Templates:  \n",
    "- Clone this repository into your project directory: [https://github.com/outerbounds/metaflow-tools](https://github.com/outerbounds/metaflow-tools)  \n",
    "`git clone git@github.com:outerbounds/metaflow-tools.git` or `git clone https://github.com/outerbounds/metaflow-tools.git`. \n",
    "\n",
    "Feel free to remove the aws and azure folders if you want to clean up.\n",
    "\n",
    "Create a Project in GCP and Enable Services: \n",
    "\n",
    "To see which services are already enabled you can use \n",
    "\n",
    "`gcloud services list`\n",
    "\n",
    "- Enable these APIs:  \n",
    "    - Cloud Resource Manager: `gcloud services enable cloudresourcemanager.googleapis.com`  \n",
    "    - Compute Engine API:  `gcloud services enable compute.googleapis.com`  \n",
    "    - Service Networking: `gcloud services enable servicenetworking.googleapis.com`  \n",
    "    - Cloud SQL Admin API: `gcloud services enable sqladmin.googleapis.com`    \n",
    "    - Kubernetes Engine API: `gcloud services enable container.googleapis.com`  \n",
    "\n",
    "Provision and Deploy Infrastructure in GCP:  \n",
    "- Login to GCP `gcloud auth application-default login`    \n",
    "- Initialize Terraform: From `metaflow-tools/gcp/terraform` run `terraform init`    \n",
    "- Set some Terraform variables: Create a file called `metaflow.tfvars` and add the following to it:\n",
    "\n",
    "```\n",
    "org_prefix = \"<ORG_PREFIX>\"\n",
    "project = \"<GCP_PROJECT_ID>\"\n",
    "enable_argo=true\n",
    "```\n",
    "\n",
    "ORG_PREFIX should just be a short unique name, this will be the name of the GCS bucket. GCP_PROJECT_ID is the project ID you created in GCP.\n",
    "\n",
    "- Provision the GCP infrastructure using Terraform. From `metaflow-tools/gcp/terraform` run `terraform apply -target=\"module.infra\" -var-file=metaflow.tfvars`    \n",
    "- Deploy services using Terraform. From `metaflow-tools/gcp/terraform` run `terraform apply -target=\"module.services\" -var-file=metaflow.tfvars` \n",
    "\n",
    "In the above we use the `-target` option to target which parts of the configuration to *apply*. Supposedly it is easier to troubleshoot errors this way, and you can be sure to apply certain parts of the infrastructure first that might be needed by other parts of the infrastructure. \n",
    "\n",
    "Configure Metaflow to Work with GCP and Kubernetes:  \n",
    "- Verify we can access our infrastructure:\n",
    "\n",
    "```\n",
    "gcloud auth application-default login\n",
    "gcloud container clusters get-credentials gke-metaflow-default --region=us-west2-a\n",
    "```\n",
    "\n",
    "You may need to install the GKE gcloud auth plugin:\n",
    "\n",
    "`gcloud components install gke-gcloud-auth-plugin`\n",
    "\n",
    "- Configure Metaflow. Create the file `~/.metaflowconfig/config.json` and add the following to it. Replace the `<org-prefix>` with the one you chose above.  \n",
    "\n",
    "```\n",
    "{\n",
    "    \"METAFLOW_DATASTORE_SYSROOT_GS\": \"gs://storage-<org-prefix>-metaflow-default/tf-full-stack-sysroot\",\n",
    "    \"METAFLOW_DEFAULT_DATASTORE\": \"gs\",\n",
    "    \"METAFLOW_DEFAULT_METADATA\": \"service\",\n",
    "    \"METAFLOW_KUBERNETES_NAMESPACE\": \"argo\",\n",
    "    \"METAFLOW_KUBERNETES_SERVICE_ACCOUNT\": \"argo\",\n",
    "    \"METAFLOW_SERVICE_INTERNAL_URL\": \"http://metadata-service.default:8080/\",\n",
    "    \"METAFLOW_SERVICE_URL\": \"http://127.0.0.1:8080/\"\n",
    "}\n",
    "```\n",
    "- Setup the port-forwards by running the following script in a new terminal. This will run the port-forwards automatically and will avoid timeouts: `python metaflow-tools/scripts/forward_metaflow_ports.py --include-argo` (keep this terminal open and continue working in a new terminal)\n",
    "- Install GCP Python SDK in your project environment: `pip install google-cloud-storage google-auth` \n",
    "\n",
    "\n",
    "One thing to be careful of, if you run all of this on a project, and then you want to run it on a new project, you may need to run some commands. You can check which project you are in by running:\n",
    "\n",
    "`gcloud config get-value project`\n",
    "\n",
    "You can see the list of available projects:\n",
    "\n",
    "`gcloud projects list`\n",
    "\n",
    "You can switch to a new project: \n",
    "\n",
    "`gcloud config set project <project_id>`  \n",
    "\n",
    "You may also need to set the quota/billing project:\n",
    "\n",
    "`gcloud auth application-default set-quota-project <project_id>`  \n",
    "\n",
    "And you can always check which project is the default quota/billing project by looking in this file:  \n",
    "\n",
    "`~/.config/gcloud/application_default_credentials.json`  \n",
    "\n",
    "\n",
    "There are also configuration files in the `~/.config/gcloud/configurations/` folder. These are created when you run `gcloud init`. You always check these to make sure the correct project is listed there, and if not, you can edit the file. You can use `gcloud config list` to see which configuration is active, and activate a configuration using `gcloud config configurations activate <config-name>`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dependencies with @conda pg 143 192\n",
    "\n",
    "If we want to truly make our flows reproducible then we should define the environment that our flows will run in. We can do this using the `@conda` or `@conda_base` decorators. With these decorators we can be very specific as to which version of python, and which versions of python libraries we are using in each step. We can even choose different versions of the same library to run in different steps.\n",
    "\n",
    "Metaflow using the conda package manager by default, and so before using these decorators you may have to set an environment variable called CONDA_CHANNELS to conda-forge in your `.bash_profile` or `.zprofile` or `.profile` or wherever your environment variables are set. I'm using zsh, so I set my environment variable in my `.zprofile` file.\n",
    "\n",
    "Add this line to the profile: `export CONDA_CHANNELS=conda-forge`  \n",
    "Then run `source ~/.zprofile`.\n",
    "\n",
    "Now, let's modify our `classifier_train.py` file to do more than just train 2 different models. We'll make the DAG dynamic by adding a `foreach` in the first step, and then we'll add a `train_lasso` step where we train a Lasso model using a set of values for `alpha`. If we create five values of `alpha`, then the flow will spin off five nodes in the DAG. Let's set all of our dependencies using `@conda_base`, but note you can also do this separately for each step in the flow as well.\n",
    "\n",
    "```\n",
    "from metaflow import FlowSpec, step, conda_base\n",
    "\n",
    "@conda_base(libraries={'numpy':'1.23.5', 'scikit-learn':'1.2.2'}, python='3.9.16')\n",
    "class ClassifierTrainFlow(FlowSpec):\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "        from sklearn import datasets\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        import numpy as np\n",
    "\n",
    "        X, y = datasets.load_wine(return_X_y=True)\n",
    "        self.train_data, self.test_data, self.train_labels, self.test_labels = train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "        print(\"Data loaded successfully\")\n",
    "        self.lambdas = np.arange(0.001, 1, 0.2)\n",
    "        self.next(self.train_lasso, foreach='lambdas')\n",
    "\n",
    "    @step\n",
    "    def train_lasso(self):\n",
    "        from sklearn.linear_model import Lasso\n",
    "\n",
    "        self.model = Lasso(alpha=self.input)\n",
    "        self.model.fit(self.train_data, self.train_labels)\n",
    "        self.next(self.choose_model)\n",
    "\n",
    "    @step\n",
    "    def choose_model(self, inputs):\n",
    "        def score(inp):\n",
    "            return inp.model, inp.model.score(inp.test_data, inp.test_labels)\n",
    "\n",
    "        self.results = sorted(map(score, inputs), key=lambda x: -x[1])\n",
    "        self.model = self.results[0][0]\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        print('Scores:')\n",
    "        print('\\n'.join('%s %f' % res for res in self.results))\n",
    "        print('Model:', self.model)\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ClassifierTrainFlow()\n",
    "```\n",
    "\n",
    "Once the above script is saved as `classifier_train_more.py`, you can run it using `python classifier_train_more.py --environment=conda run`. This will take a little bit of time while the environment is being set up. The next time you run it, it will run much faster since all of the dependencies have already been installed.\n",
    "\n",
    "In the next step, we'll run the same flow in kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### @kubernetes and @resources\n",
    "\n",
    "You can test if things are working by running: `python classifier_train_more.py --environment=conda run --with kubernetes`. It may take some time to create all of the containers and run everything, but it *should* be successful. You may want to modify the resources using the `@resources` decorator to see if it has any effect on the performance, but for our dataset here, which is small, it will likely not make much difference.\n",
    "\n",
    "Don't be surprised that the flow takes a lot longer to run in kubernetes. It runs quickly on your laptop because the data is small and the computation is easy. In kubernetes there is a lot of overhead involved in creating pods and containers and such. But, let's run a few more models anyways, just to continue to test our flows running in kubernetes. Modify the `classifier_train_more.py` script by changing the following line:\n",
    "\n",
    "`self.lambdas = np.arange(0.001, 1, 0.2)`\n",
    "\n",
    "to \n",
    "\n",
    "`self.lambdas = np.arange(0.001, 1, 0.01)`\n",
    "\n",
    "This code will attempt to train more models. It will still run fast on your laptop, and slow in kubernetes, but that's ok. Kubernetes is where you want things to run in production so that it *can* scale and deal with failures.\n",
    "\n",
    "### @timeout, @retry and @catch \n",
    "\n",
    "Note that anytime you are running flows in an orchestrator, you should use the `@retry` decorator to avoid any transient infrastructure issues so that the step will terminate and run again if nothing is happening. You can also add a `@timeout` decorator to any steps that you feel should run in under a certain amount of time. Lastly, you can use the `@catch` decorator to tell Metaflow what to do when an exception is caught.\n",
    "\n",
    "You can also include the `retry` when you run the flow, like this: `python classifier_train_more.py --environment=conda run --with retry`. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scheduling with @schedule\n",
    "\n",
    "Metaflow is not a scheduler. Unfortunately, if you want to run a flow in production you will need to take your flow and run it in a separate scheduler, such as Airflow or Argo Workflows. The way we set up our infrastructure on GCP above, we will use Argo Workflows to schedule our flows to run.\n",
    "\n",
    "First, let's add `@schedule(hourly=True)` to the top of our flow, and save the script as `classifier_train_prod.py`.\n",
    "\n",
    "Then we can create a new Argo Workflow by running \n",
    "\n",
    "`python classifier_train_prod.py --environment=conda --with retry argo-workflows create`\n",
    "\n",
    "After you run this command you can go into the Argo server (remember those port-forwards we did up above). In the Argo UI click `Submit New Workflow`, then find the flow you just created, and hit `Submit`. You should then see the flow running. \n",
    "\n",
    "If we run the above `create` command again, it will create a new version of the workflow, and this will become the new production version. If we want to test our flow (stage it) before we put it in production we can simply run it in kubernetes without creating the Argo workflow, or we can change the name of the flow from ClassifierTrainFlow to ClassifierTrainFlowStaging, create an Argo workflow, and make sure it runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Metaflow Cheatsheet\n",
    "\n",
    "- Flows or workflows: same as a DAG  \n",
    "- `FlowSpec`: metaflow class for flows  \n",
    "- `@step`: defines a node in the DAG  \n",
    "- `start` and `end`: required steps  \n",
    "- `next(step_name)`: defines edges between nodes  \n",
    "- artifacts: variables, data, models   \n",
    "- `self`: use to persist all artifacts  \n",
    "- parameters: for setting parameter values for runs of the flow  \n",
    "- branching and merging: for creating more complex DAGs  \n",
    "- @conda: for dependency management  \n",
    "- @resources: for specifying size of instances in compute layer  \n",
    "- @kubernetes (@batch): for running steps in the cloud  \n",
    "- @timeout, @retry, and @catch: for dealing with errors  \n",
    "- @schedule: for scheduling flows to run in Argo Workflows (or Airflow)  \n",
    "- decorators versus --with: rather than hard-code decorators we can include --with (e.g. --with retry or --with kubernetes) to make flows a little more flexible  \n",
    "- argo_workflows create: to create a workflow to run and schedule in Argo Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Workflows Lab\n",
    "\n",
    "## Overview\n",
    "\n",
    "We've learned that a machine learning pipeline, or flow, makes it easy to train and score ML models. But, so far we've only worked on this locally. The real power of ML pipelines is scalability, i.e. in training a model that requires a lot of resources such as data or compute easily, in the cloud, without having to worry about setting up the infrastructure ourselves. ML pipelines can also be useful for model deployment, which we'll cover next time, and model scoring, when we need to worry about making sure our deployed model is always available, and can handle whatever we throw at it. \n",
    "\n",
    "## Goal\n",
    "\n",
    "In this lab we will set up a scalable machine learning training pipeline using Metaflow and GCP and then test our ML training pipeline code from the previous lab.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Use the instructions above to get Metaflow deployed in GCP. You may want to start with a fresh project to test it out, and then you can try running Metaflow and MLFlow in the same GCP project. After you've tested it out, get it running along with your MLFlow Server instance and rerun your training flow from the previous lab using kubernetes on GCP instead of your laptop. \n",
    "\n",
    "### Scaling your Training Flow\n",
    "\n",
    "1. Set up Metaflow to run in GCP on K8s using the instructions above.   \n",
    "    a. Feel free to test it out on a new clean project before setting it up with your MLFlow-related project.  \n",
    "2. Create copies of the files you created in Lab 6 and rename them trainingflowgcp.py and scoringflowgcp.py. Add the `@resources` or `@kubernetes` decorators to your flow where they make the most sense (when you need the step to be truly scalable or you can just scale out the entire flow).  \n",
    "3. Add the `@timeout`, `@retry`, and `@catch` decorators to your flow to avoid errors.  \n",
    "4. Add the `@conda` or `@conda_base` decorators to control dependencies.  \n",
    "5. Run your ML training pipeline in GCP. **Take a screenshot** of your terminal that shows that your flow ran successfully in GCP. Be sure that a final model is registered in MLFlow as part of the flow and **take a screenshot** of the newly registered model in the MLFlow dashboard. You **do not** need to use Argo for anything.      \n",
    "6. To save on costs, once you are all done and have turned in the assignment, I would recommend using `terraform destroy` to destroy the entire infrastructure. You can always get it up and running again using the `terrafrom apply` commands above.\n",
    "\n",
    "### Turning It In\n",
    "\n",
    "To show that your model training flow runs successfully in Kubernetes in GCP, take a screenshot of some of the output in the terminal after you `run` your model training flow. Be sure to include the end that shows it was successful and didn't error out.  \n",
    "\n",
    "Take a screenshot of your new registered model in MLFlow.\n",
    "\n",
    "Push your new python files that contain your model training flow to Github. In a doc, add the direct link to the `src/` folder in Github, add the two screenshots, and submit as a pdf to Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
