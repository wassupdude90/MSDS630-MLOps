{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "There are many approaches to deploying machine learning models. How models get deployed will depend on several things, such as existing infrastructure, how the model predictions will be used, and how quickly they are needed. There is so much variation that it is impossible to be 100% prepared for every scenario, but we can look at the most common deployment patterns and at least get some practice with each one using a fairly simple ML model.\n",
    "\n",
    "There are two primary patterns for deploying models: offline and online. Offline deployment is commonly referred to as either batch or asynchronous processing, while online deployment is referred to as either real-time or synchronous. Both patterns can be very flexible and complex depending on where the input data is coming from *upstream* and how the predictions will actually be used *downstream*. For the purposes of these demos we will either assume that there are already data pipelines in place that consume raw data and store it (for our offline deployments) and that there is some downstream application that consumes the predictions from the model (also mostly for our offline deployments). For our real-time demo, we will input the data directly to the client (the webapp) and the results will be received directly by us.\n",
    "\n",
    "The number of technologies and libraries that we can use for our demos is vast, but we will stick to just a handful of simple to use methods for creating quick prototypes. \n",
    "\n",
    "## Our Trained Model\n",
    "\n",
    "For the purposes of this demo I have already trained a very simple model for doing text classification. This model was trained using this [reddit](https://raw.githubusercontent.com/axsauze/reddit-classification-exploration/master/data/reddit_train.csv) dataset which consists of Reddit comments and whether they were removed or not (1 = removed, 0 = not removed). The model is a simple logistic regression model, nothing fancy, and not really important for our purposes. There are two steps to the model: the Reddit comment is processed and vectorized, and then the vector is passed to the logistic regression model.\n",
    "\n",
    "In order to use the model we need the pipeline that vectorizes the text and contains our model. This pipeline is in Canvas. In practice, we can store these in a model registry using something like MLFlow, or they might already be stored in a datastore if we used Metaflow. Using a registry would be more appropriate for versioning the models but for this demo we'll simply store everything, all data, artifacts, this notebook, and scripts in a new `reddit/` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data\n",
    "\n",
    "We do have a sample of data which we can use to test out our offline deployment. The sample of data can also be found in Canvas, and we can save it in our `data/` folder (or wherever we want so long as we remember where it is) as 'sample_reddit.csv'. Again, to test the deployment locally, this is fine. But if we were going to truly productionize this, then we should have our batch data sitting in the actual data warehouse or object storage in a *production* environment.\n",
    "\n",
    "After our model makes predictions we need to write those predictions somewhere. We could set up a sqlite database pretty easily and put them there, but again, let's just dump them into a file for now and not worry about writing all of the code to set up the database, the table, and the queries to insert the new predictions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Offline Deployment\n",
    "\n",
    "For offline deployment we need to create a scoring script that will do these steps: (1) load data; (2) clean and process data; (3) load model; (4) make predictions; (5) store predictions. We will **load data** from our filesystem, we will **load the model** from our filesystem, and we will **store predictions** to our filesystem. But, if we wanted to, we could make this more robust by storing the input and output data in sqlite tables, and have the model sitting in a datastore or registry created by MLflow or Metaflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scoring Script\n",
    "\n",
    "We have a few choices even for how we can write our scoring script. We can simply write a python script that does the above steps in order. We can create flows using Metaflow (or Prefect or something else) which will save all of the flow metadata when the flow is run. How this is done will depend on the team we're working on, but for this demo, let's go ahead and continue to use Metaflow to create a flow that we can run at scoring time and schedule to run using a scheduler such as Airflow or Argo Workflows. We won't do the scheduling part for this demo.\n",
    "\n",
    "For this demo we will need to install the joblib library for loading our model pipeline.\n",
    "\n",
    "```\n",
    "pip install joblib\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our sample dataset so that we can write a scoring script and test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2134,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "x_sample = pd.read_csv('sample_reddit.csv', header=None).to_numpy().reshape((-1,))\n",
    "x_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "class RedditClassifier(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        self._loaded_pipeline = joblib.load(\"reddit_model_pipeline.joblib\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = self._loaded_pipeline.predict_proba(X)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"forced to wake up too early most students are in a rush, then don't eat, then go to school with no breakfast and can't focus or study. It has never made any sense\"]\n",
      "[[0.55795308 0.44204692]]\n"
     ]
    }
   ],
   "source": [
    "sample = x_sample[2000:2001]\n",
    "print(sample)\n",
    "print(RedditClassifier().predict(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above class should give us a set of predictions for the given `sample` of data. It works ok, but let's break it down into a flow using Metaflow that makes more sense, and that is more reproducible, saves the metadata, and can run in Kubernetes with a simple addition of using the `run --with kubernetes` command."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from metaflow import FlowSpec, step, conda_base\n",
    "@conda_base(packages={'joblib':'1.4.2','numpy':'2.2.5', 'scikit-learn':'1.6.1', 'pandas':'2.2.3'}, python='3.13.3')\n",
    "\n",
    "class RedditClassifier(FlowSpec):\n",
    "    @step\n",
    "    def start(self):\n",
    "        print(\"Flow is starting\")\n",
    "        self.next(self.load_data)\n",
    "\n",
    "    @step\n",
    "    def load_data(self):\n",
    "        import pandas as pd\n",
    "        import numpy\n",
    "\n",
    "        print(\"Data is loading\")\n",
    "        self.x_sample = pd.read_csv('sample_reddit.csv', header=None).to_numpy().reshape((-1,))\n",
    "        print(\"Data is loaded\")\n",
    "        self.next(self.load_model)\n",
    "\n",
    "        \n",
    "    @step\n",
    "    def load_model(self):\n",
    "        import joblib\n",
    "        print(\"Pipeline loading\")\n",
    "        self.loaded_pipeline = joblib.load(\"reddit_model_pipeline.joblib\")\n",
    "        print(\"Pipeline loaded\")\n",
    "        self.next(self.predict_class)\n",
    "\n",
    "    @step\n",
    "    def predict_class(self):\n",
    "        print(\"Making predictions\")\n",
    "        self.predictions = self.loaded_pipeline.predict_proba(self.x_sample)\n",
    "        print(\"Predictions made\")\n",
    "        self.next(self.save_results)\n",
    "\n",
    "\n",
    "    @step\n",
    "    def save_results(self):\n",
    "        import pandas as pd\n",
    "        print(\"Saving results\")\n",
    "        pd.DataFrame(self.predictions).to_csv(\"sample_preds.csv\", index=None, header=None)\n",
    "        print(\"Results saved\")\n",
    "        self.next(self.end)\n",
    "        \n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        print(\"Flow is ending\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    RedditClassifier()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above flow should work without error, though there is definitely room for improvement. For example, instead of reading and writing csv files, we would read from, and write to, our production data stores. And instead of loading models using `joblib` we could use a model registry, or we could've used metaflow to train the model in a flow called `RedditClassifierTrain`, and then use `Flow('RedditClassifierTrain').latest_run` to grab the artifacts from the latest run of the training flow. We could also pass parameters to the flow using `Parameter`, which we might use for grabbing specific batches of data using a datetime range or something like that.\n",
    "\n",
    "One benefit of using metaflow to do the batch scoring is that each run will have a unique run ID, and we can persist any artifacts that we need. We can also easily test the batch scoring in separate dev, stage, and prod environments assuming that we have dev and stage mirrors of the prod data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Deployment\n",
    "\n",
    "For online deployment we typically expose the model as an API using either REST or gRPC. Which one we choose is not that important for our purposes here, just know that there are two main options. gRPC may be better for *larger* models, and is included in libraries like torchserve and Tensorflow Serving. \n",
    "\n",
    "These types of APIs work by having the user, or the client, make a request to the API (*give me a prediction*) and then getting a response back (*here's the prediction*). In order to make the prediction, some data needs to be sent from the client in order for the model to use it as inputs into the model, or in order to grab the data or features that are needed for the model to make the predictions. In our demo here, we will not have any other sources of features, but we can imagine that there might be batch (precomputed) features stored in a low-latency data store, and we may even have streaming features (features computed from data streams).\n",
    "\n",
    "In order to make a request to the API we use the URI, which will be in a format similar to this:\n",
    "\n",
    "`https://localhost:8000/models/{modelId}/?filter=passed#details`\n",
    "\n",
    "### Basic App with FastAPI \n",
    "\n",
    "Let's start by simply deploying the model locally. For this, we will not use the same metaflow code from above, we'll pull out the pieces and just put them inside regular python functions, and ignore the pieces we no longer need. Flask is commonly used for a job like this, but let's use [FastAPI](https://fastapi.tiangolo.com/). We're going to need the `fastapi` and `uvicorn` libraries. `uvicorn` is what we use to actually launch our app, a simple web server. Read more about it [here](https://www.uvicorn.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a separate `app/` folder we can save the below in a file called `redditApp.py`. Within the code we call `FastAPI` and provide some metadata about the app first, and then we'll define a couple of very simple endpoints using the `@app.get()` decorator that will simply return some text at the given resource path.\n",
    "\n",
    "```\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Reddit Comment Classifier\",\n",
    "    description=\"Classify Reddit comments as either 1 = Remove or 0 = Do Not Remove.\",\n",
    "    version=\"0.1\",\n",
    ")\n",
    "\n",
    "# Defining path operation for root endpoint\n",
    "@app.get('/')\n",
    "def main():\n",
    "\treturn {'message': 'This is a model for classifying Reddit comments'}\n",
    "\n",
    "# Defining path operation for /name endpoint\n",
    "@app.get('/{name}')\n",
    "def hello_name(name : str):\n",
    "\treturn {'message': f'Hello {name}'}\n",
    "```\n",
    "\n",
    "With that saved in the `app` directory, we can run this command using `uvicorn` in the terminal (from the `app` directory) to test it out:\n",
    "\n",
    "`uvicorn redditApp:app --reload`\n",
    "\n",
    "Running this, and then going to http://127.0.0.1:8000, we should see the following:\n",
    "\n",
    "`{\"message\":\"This is a model for classifying Reddit comments\"}`\n",
    "\n",
    "And if we instead go to http://127.0.0.1:8000/Clementine, we should see the following:\n",
    "\n",
    "`{\"message\":\"Hello clementine\"}`\n",
    "\n",
    "Our very simple app is working and we're able to return some text in JSON format by simply visiting the URL. Let's move on and do something a bit more complicated now.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Modify App to do ML Predictions\n",
    "\n",
    "Ok, now let's modify the script to include our model, a request body (a Reddit comment), and the response (the predictions). For this, we'll need to import all of the libraries that we need in order to run our model. Then we'll need to create a request body. The request body is essentially the request we will make to our endpoint in order to get a response back containing the predictions. For this app, the request can simply be a string, a Reddit comment.\n",
    "\n",
    "From here, we can use much of the code we've already written above. We'll first load the pipeline at the startup of the app using the `@app.on_event('startup')` decorator. Then we create a new function that will take the comment and generate and return a prediction. This function will take the Reddit comment in the `request_body`, make the predictions from our logistic regression model, and then return a dictionary which will be converted to JSON. We also add a new endpoint resource operator using the `@app.post('/predict')` decorator, which we'll notice uses the **post** method rather than the **get** method. For more details on the difference between **post** and **get** (and **put** and **delete**) we can look [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods), but in general with **get** we are receiving data and with **post** we will submit data which will alter the state of the server.\n",
    "\n",
    "```\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "import joblib\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Reddit Comment Classifier\",\n",
    "    description=\"Classify Reddit comments as either 1 = Remove or 0 = Do Not Remove.\",\n",
    "    version=\"0.1\",\n",
    ")\n",
    "\n",
    "# Defining path operation for root endpoint\n",
    "@app.get('/')\n",
    "def main():\n",
    "\treturn {'message': 'This is a model for classifying Reddit comments'}\n",
    "\n",
    "class request_body(BaseModel):\n",
    "    reddit_comment : str\n",
    "\n",
    "@app.on_event('startup')\n",
    "def load_artifacts():\n",
    "    global model_pipeline\n",
    "    model_pipeline = joblib.load(\"reddit_model_pipeline.joblib\")\n",
    "\n",
    "\n",
    "# Defining path operation for /predict endpoint\n",
    "@app.post('/predict')\n",
    "def predict(data : request_body):\n",
    "    X = [data.reddit_comment]\n",
    "    predictions = model_pipeline.predict_proba(X)\n",
    "    return {'Predictions': predictions}\n",
    "```\n",
    "\n",
    "One very nice thing about FastAPI is that it automatically creates a docs resource, which comes in very handy for testing out our new API. When we run the app we can go to http://127.0.0.1:8000/docs, and we'll see something like this:\n",
    "\n",
    "\n",
    "\n",
    "If we go to the POST section and click the down arrow we'll be able to try out an API request by hitting the `Try it out` button:\n",
    "\n",
    "\n",
    "\n",
    "Then we can input our own comment and hit execute:\n",
    "\n",
    "\n",
    "\n",
    "And then we can scroll down to see the predictions made for our comment:\n",
    "\n",
    "\n",
    "\n",
    "Notice the `curl` command that was created when we hit execute which shows the method **POST**, the endpoint and resource `http://127.0.0.1:8000/predict`, the headers (`-H`, which basically say we are sending and receiving JSON), and then the body (`-d`, JSON object which has our comment that we want to get predictions on). \n",
    "\n",
    "### Test App with Python\n",
    "\n",
    "Instead of using `curl` we can also test the app using a simple python script and the `requests` library. Let's create a script that can test the app for us. Copy the code below and save it as `test_app.py` and then we can run it in the terminal using `python test_app.py`, as long as our app is actually running, to see if it works. Or you can simply run the code block below in this notebook and see if you get a set of predictions back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Predictions': {'0.3524878769638867': 0.6475121230361133}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "comment = {'reddit_comment':'Testing a comment.'}\n",
    "\n",
    "url = 'http://127.0.0.1:8000/predict'\n",
    "response = requests.post(url, json=comment)\n",
    "print(response.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containerize\n",
    "\n",
    "At this point we should consider putting our app into a container. This way we can share it, and truly think about scaling it out. We can do this part ourselves by writing a Dockerfile and then building and running the container locally. Recall that in order to create a Dockerfile, we just need to go step-by-step and figure out what it is that we need in our container so that the app will run.\n",
    "\n",
    "First, we need a python base image. I will use my version of python that I've been using, which is 3.13.3.  \n",
    "Then, we need our list of requirements so that we can install them. For the app we used:\n",
    "\n",
    "```\n",
    "scikit-learn==1.6.1\n",
    "numpy==2.2.5\n",
    "uvicorn==0.34.2\n",
    "fastapi==0.115.12\n",
    "joblib==1.4.2\n",
    "pydantic==2.11.3\n",
    "```\n",
    "\n",
    "Then, we should change the working directory and copy over the files that we need for our app to run.    \n",
    "\n",
    "Then, we need to expose the port.  \n",
    "\n",
    "And lastly we tell Docker what to actually **run** when we run the container.  \n",
    "\n",
    "Let's build out the dockerfile and see how it goes.\n",
    "\n",
    "`touch Dockerfile`\n",
    "\n",
    "And now add the following to the Dockerfile.\n",
    "\n",
    "```\n",
    "FROM python:3.13-slim\n",
    "\n",
    "RUN pip install -U pip \n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY [ \"requirements.txt\", \"./\" ]\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY [ \"reddit_model_pipeline.joblib\", \"redditApp.py\", \"./\"]\n",
    "\n",
    "EXPOSE 8000\n",
    "\n",
    "# --host=0.0.0.0 is necessary when using uvicorn inside a docker container so that you can connect to localhost or 127.0.0.1 from outside of container\n",
    "ENTRYPOINT [ \"uvicorn\", \"redditApp:app\", \"--host\", \"0.0.0.0\"]\n",
    "```\n",
    "\n",
    "Once the Dockerfile and the requirements.txt file is written, we can build and run the app:\n",
    "\n",
    "`docker build -t redditapp:v1 .`  \n",
    "`docker run -it -p 8000:8000 redditapp:v1`\n",
    "\n",
    "And then we can run the `test_app.py` file to test if our app is actually running. Ok, so our app is running in a container, which is cool. We can use curl commands, like below, and get predictions back. But, it was a little bit of work getting our Dockerfile to work correctly, and it's still not an ideal way to share with others that might need to test out the application. Having a simple UI, where we can get others to quickly go and test out the app, and provide feedback, might be very useful.\n",
    "\n",
    "```\n",
    "curl -X 'POST' \\\n",
    "'http://127.0.0.1:8000/predict' \\\n",
    "-H 'accept: application/json' \\\n",
    "-H 'Content-Type: application/json' \\\n",
    "-d '{\"reddit_comment\": \"Useless comment, you should flag it for removal\"}'\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a UI Using Streamlit\n",
    "\n",
    "Another option we have for creating a very quick prototype of an application that we can interact with in order to test out our model, or to share with others on our team so that they can test it out, is to build a *data app*, or a dashboard, and expose the model that way with a simple UI. This is fairly easy to do, and there are several tools available that we can use, such as Dash and Streamlit. For those of us who like to work on the frontend, we could also just create a UI and keep our app in FastAPI or use Flask instead. For this demo, let's use Streamlit. Before we start, we should check out the docs [here](https://docs.streamlit.io/library/get-started), and take a look at the API reference. We'll quickly see how simple Streamlit is to use.\n",
    "\n",
    "First, let's install Streamlit, and make a new `streamlit/` folder in our project root folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our `streamlit/` folder, let's create a new file called `app.py`.\n",
    "\n",
    "`touch app.py`  \n",
    "`streamlit run app.py`\n",
    "\n",
    "You won't see much yet, we'll build it out in due time. \n",
    "\n",
    "The first step when building out the UI for our app is to think about the design. We should not just jump into writing the code. We should take a step back, and think about what it should look like. We should draw some diagrams and think about the different personas that will be using the app. But, we should also remember that this is a prototype, so we shouldn't go overboard designing an amazingly slick app. I like to think about the app as two steps: the UI and the Server. This comes from my days of using [Shiny in R](https://shiny.rstudio.com/) which has been around for more than a decade. Thinking about the UI first will help us get something on the screen quickly, and help us figure out how to write the code to actually produce the outputs to the screen that we want to see.\n",
    "\n",
    "For the UI, for this particular app, all we really have is a simple text input and then the predictions as output. It's not terribly complicated, but we can make it more fancy. What if we want to upload a csv file that has a batch of comments in it and generate a prediction for each comment in the file? What if we want to download a csv file that has the comments with the predictions appended? What if we want to see other details of the model within the app, such as performance metrics, or some chart that tells us about the performance of the model on a testing set of data? We could add all of these things in pieces and see how it goes. First, we'll need the input text field, and an output that displays the predictions to us. \n",
    "\n",
    "We should start with copying over pipeline file that we need for the app to run to our new `streamlit/` folder. Then, in our `app.py` we can add the following code below. This is the bare minimum that is needed. We have a title using `st.title()`. We have some simple description using `st.markdown()`. We create an area for inputting our text using `st.text_area()`. Then we have the code we need to actually take that text and generate a prediction, and we output the prediction to the screen using `st.write()`. That's all there is to it to make the most basic of apps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import joblib\n",
    "\n",
    "st.title(\"Reddit Comment Classification\")\n",
    "st.markdown(\"### All you have to do to use this app is enter a comment and hit the Predict button.\")\n",
    "\n",
    "reddit_comment = [st.text_area(\"Input your comment here:\")]\n",
    "\n",
    "def load_artifacts():\n",
    "    model_pipeline = joblib.load(\"reddit_model_pipeline.joblib\")\n",
    "    return model_pipeline\n",
    "\n",
    "model_pipeline = load_artifacts()\n",
    "\n",
    "def predict(reddit_comment):\n",
    "    X = reddit_comment\n",
    "    predictions = model_pipeline.predict_proba(X)\n",
    "    return {'Predictions': predictions}\n",
    "\n",
    "preds = predict(reddit_comment)\n",
    "st.write(preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the above in `app.py` and run it using `streamlit run app.py`. The app will open in a browser and you can go in and enter a comment and see the prediction, and it will look like something like this: \n",
    "\n",
    "![](images/streamlit1.png)\n",
    "\n",
    "We can continue to build off of this by changing the way we write the results out to the screen. Rather than output a JSON object, let's actually output the prediction with a description using `st.metric()`. Then, let's add a file upload button using `st.file_uploader()` so that we can get predictions for a batch of comments. We'll also add a download button using `st.download_button()` so that we can download the results of our batch predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import joblib\n",
    "\n",
    "st.title(\"Reddit Comment Classification\")\n",
    "st.markdown(\"### All you have to do to use this app is enter a comment and hit the Predict button.\")\n",
    "\n",
    "reddit_comment = [st.text_area(\"Input your comment here:\")]\n",
    "\n",
    "def load_artifacts():\n",
    "    model_pipeline = joblib.load(\"reddit_model_pipeline.joblib\")\n",
    "    return model_pipeline\n",
    "\n",
    "model_pipeline = load_artifacts()\n",
    "\n",
    "def predict(reddit_comment):\n",
    "    X = reddit_comment\n",
    "    predictions = model_pipeline.predict(X)\n",
    "    return predictions \n",
    "\n",
    "preds = predict(reddit_comment)\n",
    "st.metric(\"Should this comment be removed (0: No; 1: Yes)\", preds.round(2))\n",
    "\n",
    "st.header(\"Get a Batch of Predictions\")\n",
    "\n",
    "batches = st.file_uploader(\"Upload File\", type='csv')\n",
    "\n",
    "if batches is not None:\n",
    "    dataframe = pd.read_csv(batches, header=None).to_numpy().reshape((-1,))\n",
    "    batch_predictions = pd.DataFrame(predict(dataframe))\n",
    "    batch_predictions[\"Comment\"] = dataframe\n",
    "    batch_predictions.rename(columns={0:\"Keep\", 1:\"Remove\"}, inplace=True)\n",
    "    st.write(batch_predictions)\n",
    "    st.download_button('Download Predictions', data=batch_predictions.to_csv().encode('utf-8'), file_name='predictions.csv', mime='text/csv',)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that is looking a lot better, and is simple enough to work. We don't need to clutter it up with anything. We can share the app and get feedback and add to the app, or change it, based on the feedback we get, until we get to a final version that we can either try to deploy, or to develop into a more *production-worthy* application.\n",
    "\n",
    "There are multiple ways to deploy the app, which we can read about [here](https://docs.streamlit.io/knowledge-base/tutorials/deploy). There is a Streamlit Community Cloud that will host the apps for you for free. We can also deploy in the cloud, for example with Google App Engine. Regardless of how we deploy it, we should containerize our app using Docker. \n",
    "\n",
    "Let's create the Dockerfile similar to what we did when we used FastAPI. We can use a lot of the same lines from the Dockerfile and change the last entry for `ENTRYPOINT`.\n",
    "\n",
    "```\n",
    "FROM python:3.13-slim\n",
    "\n",
    "RUN pip install -U pip \n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY [ \"requirements.txt\", \"./\" ]\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY [ \"reddit_model_pipeline.joblib\", \"app.py\", \"./\"]\n",
    "\n",
    "EXPOSE 8000\n",
    "\n",
    "ENTRYPOINT [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\n",
    "```\n",
    "\n",
    "And we also need to change the entries of our `requirements.txt` file, and save this one in the `streamlit/` folder. \n",
    "\n",
    "And then we can build the image and run our app using:\n",
    "\n",
    "`docker build -t streamlitapp .`  \n",
    "`docker run -it -p 8501:8501 streamlitapp`\n",
    "\n",
    "To be fair, Streamlit apps are really not meant for being put in production. Streamlit is great for prototyping, but when it comes time to productionize any app that might get significant traffic, it's best to build that out separately. In the next section we'll abandon Streamlit and use a tool that was built for creating scalable REST or gRPC APIs for ML models, and try to deploy it on Kubernetes in GCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment Lab\n",
    "\n",
    "## Overview\n",
    "\n",
    "There are many ways to deploy a model, but they all fall in two categories: offline and online. The difference between the two is that offline models are not running live - they are executed on a schedule or they are triggered by an even. Online models are always running, and waiting/listening for either events to occur or a request. \n",
    "\n",
    "Offline model deployments are extremely flexible, and can be accomplished with different combinations of tools for the compute layer and the scheduler. Online deployments can also be somewhat flexible, but the tools used will depend highly on how the predictions will be used. \n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this lab is to create a real-time model scoring process for our dataset that we've been using in previous labs. Although an offline deployment might make more sense for your dataset, we will use this lab to test out an online deployment, using FastAPI. Previously, we used Metaflow to create a scoring flow, which could be used in an offline deployment if we wanted to, by triggering it to run using a simple scheduler, or by scaling it out using Argo Workflows.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Create a basic web app for your model using FastAPI. It should:\n",
    "\n",
    "- take a request   \n",
    "- load your model from MLFlow, any one that you've built in a previous lab     \n",
    "- return a prediction   \n",
    "\n",
    "and it should work locally using a curl command and using a `test_app.py` script which uses the requests library. There is no need to create a streamlit app for this lab.\n",
    "\n",
    "Create a new `lab8app/` folder, and add your app script and `test_app.py` to it. Use a curl command to get a prediction and **take a screenshot** of the command and the output. Run `test_app.py` and **take a screenshot** of the output. Lastly, push your code to Github.\n",
    "\n",
    "## Turning It In\n",
    "\n",
    "In a doc add the Github URL direct to the `lab8app/` folder, add the two screenshots from above, and upload to Canvas as a pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
